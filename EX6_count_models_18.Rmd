---
title: "DEM 7283 - Example 4 - Count Data Models for individual and aggregate data"
author: "Corey S. Sparks, Ph.D."
date: "February 19, 2018"
output:
  html_document: default
  pdf_document: 
    latex_engine: xelatex
---

This example will cover the use of R functions for fitting count data models to complex survey data and to aggregate data at the county level. Specifically, we focus on the Poisson and Negative Binomial models to individual level survey data as well as for aggregate data.

For this example I am using 2016 CDC Behavioral Risk Factor Surveillance System (BRFSS) SMART county data. [Link](https://www.cdc.gov/brfss/smart/smart_2016.html)

We will also use data from the CDC [Compressed mortality file](https://wonder.cdc.gov/mortsql.html) on mortality in US counties. 


###Poisson model for counts
For the Poisson model, you observe some count of events. The Poisson distribution has only one parameter, $(\lambda)$, which is the average count (y). 


The Poisson GLM assumes:
$$Y \sim Poisson (\lambda),\text{   } E(Y) = \lambda,\text{   } var(Y) = \lambda$$
$$\lambda = log(\eta)$$

$$\eta = \beta_0 + \beta_1 x_1+ log(n)$$
Which is the log linear model for a count, with the potential for adding an offset term to them model. We have several ways of modeling the Poisson count: 

_Pure count_ If each area has the same risk set. This means each observation is exposed to the "process" for the same amount of time, this is the *exposure* level. If observations do not all have the same level of exposure, then we may express the count as a _Rate_. If we do this, we need to include an offset term in your model to incorporate unequal risk. I.e. people who are exposed to a disease for a year are more likely to experience the disease than someone who is exposed for a week.

$$log(y)= X' \beta + log(n)$$

, where n is the period of risk. This is called the _offset_ term in the model.

_Standardized ratio_ incorporate differential exposure as an expected count 

$$log(y)= X' \beta + log(E)$$.

In Epidemiology, the ratio $\frac{y}{E}$ is referred to as the standardized mortality (or incidence) ratio. In order to get the expected counts for an area, typically the population of the area is multiplied by the "global rate". For US county mortality, we would multiply the population of each county by the US mortality rate. 

$$E = r * n$$
$$r = \frac{\sum y}{\sum n}$$


#Interpreting the model parameters
All interpretation of parameters is done on a log scale, so 

$exp(\beta) = \text{% change in the mean}$

, or % change in the rate, for a 1 unit change in X. All testing is done in the same manner.



```{r "setup", include=FALSE}
require("knitr")
opts_knit$set(progress = FALSE)
#opts_knit$set(root.dir = "~/Google Drive/classes/dem7283//class18/data/")
```

```{r, message=F, warning=F}
#load brfss
library(car)
library(stargazer)
library(survey)
library(sjPlot)
library(ggplot2)
load("~/Google Drive/classes/dem7283/class18/data/brfss16_mmsa.Rdata")

#The names in the data are very ugly, so I make them less ugly
nams<-names(brfss16m)
head(nams, n=10)
#we see some names are lower case, some are upper and some have a little _ 
#in the first position. This is a nightmare.
newnames<-tolower(gsub(pattern = "_",replacement =  "",x =  nams))
names(brfss16m)<-newnames


```

Our outcome here is the number of days the respondent reported poor physical health in the past month:
Q: Now thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good?
```{r}
brfss16m$healthdays<-recode(brfss16m$physhlth, recodes = "88=0; 77=NA; 99=NA")
hist(brfss16m$healthdays)
summary(brfss16m$healthdays)
```


Other variables:

```{r}
#brfss16m$badhealth<-ifelse(brfss16m$genhlth %in% c(4,5),1,0)
brfss16m$badhealth<-recode(brfss16m$genhlth, recodes="4:5=1; 1:3=0; else=NA")
#race/ethnicity
brfss16m$black<-recode(brfss16m$racegr3, recodes="2=1; 9=NA; else=0")
brfss16m$white<-recode(brfss16m$racegr3, recodes="1=1; 9=NA; else=0")
brfss16m$other<-recode(brfss16m$racegr3, recodes="3:4=1; 9=NA; else=0")
brfss16m$hispanic<-recode(brfss16m$racegr3, recodes="5=1; 9=NA; else=0")

brfss16m$race_eth<-recode(brfss16m$racegr3, 
recodes="1='nhwhite'; 2='nh black'; 3='nh other';4='nh multirace'; 5='hispanic'; else=NA",
as.factor.result = T)
brfss16m$race_eth<-relevel(brfss16m$race_eth, ref = "nhwhite")

#insurance
brfss16m$ins<-recode(brfss16m$hlthpln1, recodes ="7:9=NA; 1=1;2=0")

#income grouping
brfss16m$inc<-ifelse(brfss16m$incomg==9, NA, brfss16m$incomg)

#education level
brfss16m$educ<-recode(brfss16m$educa,
recodes="1:2='0Prim'; 3='1somehs'; 4='2hsgrad'; 5='3somecol'; 6='4colgrad';9=NA",
as.factor.result=T)
brfss16m$educ<-relevel(brfss16m$educ, ref='2hsgrad')

#employment
brfss16m$employ<-recode(brfss16m$employ1,
recodes="1:2='Employed'; 2:6='nilf'; 7='retired'; 8='unable'; else=NA",
as.factor.result=T)
brfss16m$employ<-relevel(brfss16m$employ, ref='Employed')

#marital status
brfss16m$marst<-recode(brfss16m$marital,
recodes="1='married'; 2='divorced'; 3='widowed'; 4='separated'; 5='nm';6='cohab'; else=NA",
as.factor.result=T)
brfss16m$marst<-relevel(brfss16m$marst, ref='married')

#Age cut into intervals
brfss16m$agec<-cut(brfss16m$age80, breaks=c(0,24,39,59,79,99))

#BMI, in the brfss16ma the bmi variable has 2 implied decimal places,
#so we must divide by 100 to get real bmi's

brfss16m$bmi<-brfss16m$bmi5/100

#smoking currently
brfss16m$smoke<-recode(brfss16m$smoker3, 
recodes="1:2='Current'; 3='Former';4='NeverSmoked'; else=NA", 
as.factor.result=T)
brfss16m$smoke<-relevel(brfss16m$smoke, ref = "NeverSmoked")

```

###Analysis
First, we will subset our data to have complete cases for our variables in our model and make our survey design object

```{r}
#Here I keep complete cases on my key variables,
#just for speed (the suvey procedures can run for a long time)
library(dplyr)
sub<-brfss16m%>%
  select(healthdays, mmsaname, bmi,
         agec,race_eth, marst, educ,white, black, hispanic,
         other, smoke, ins, mmsawt, ststr) %>%
  filter( complete.cases(.))

#First we tell R our survey design
options(survey.lonely.psu = "adjust")
des<-svydesign(ids=~1, strata=~ststr, weights=~mmsawt, data =sub )
#OR THE BRFSS, R GAVE ME A WARNING AND I NEEDED TO ADD:
#YOU MAY NOT NEED TO DO THIS!!!!
#First we tell R our survey design
options(survey.lonely.psu = "adjust")
des<-svydesign(ids=~1, strata=~ststr, 
               weights=~mmsawt,
               data = brfss16m[is.na(brfss16m$mmsawt)==F,])
```

##Poisson regression example
To fit a Poisson GLM to survey data in R, we use the `svyglm` function in the survey library. 

```{r}
#First I do some simple descriptives
svyhist(~healthdays, des)
svyby(~healthdays, ~race_eth+educ, des, svymean, na.rm=T)
svyby(~healthdays, ~agec, des, svymean, na.rm=T)

#Poisson glm fit to survey data
fit1<-svyglm(healthdays~factor(race_eth)+factor(educ)+factor(agec), design=des, family=poisson)
summary(fit1)
#here are the poisson model "risk ratios", which just show the change in the mean
round(exp(summary(fit1)$coef[-1,1]), 3)
```

So, we interpret this as follows. NH Multirace respondents had higher mean counts of poor health days than NH Whites, while NH Others had a lower mean count of poor health days. As education increases, the mean count of poor health days decreases. Also, as age increase, the mean count of poor health days increases. 

In terms of the risk ratios $exp(\beta)$ NH multirace respondents had 32% higher number of days when their health was poor, compared to NH whites. In practice, this translates into : `r test<-svyby(~healthdays, ~white, svymean, design =des); test[2,2]+.32*test[2,2]` days for NH multirace, and `r test<-svyby(~healthdays, ~white, svymean, design =des); test[2,2]` for NH whites. 

##Overdispersion
When using the Poisson GLM, you often run into _overdispersion_ 
* What's overdispersion? For the Poisson distribution, the mean and the variance are functions of one another (variance = mean for Poisson). So when you have more variability than you expect in your data, you have overdispersion. This basically says your data do not fit your model, and is a problem because overdispersion leads to standard errors for our model parameters that are too small. But, we can fit other models that do not make such assumptions, or allow there to be more variability. 

**An easy check on this is to compare the residual deviance to the residual degrees of freedom. They ratio should be 1 if the model fits the data.**

##NOTE
The `svyglm()` function includes a scaling term for overdispersion, so this is already taken into account. But if you have data that aren't a complex survey, we can measure this ourselves using the residual deviance.

```{r}
fit2<-glm(healthdays~factor(race_eth)+factor(educ)+factor(agec), data=brfss16m, family=poisson)
summary(fit2)
scale<-sqrt(fit2$deviance/fit2$df.residual)
scale
```

The deviance can also be a test of model fit, using a chi square distribution, with degrees of freedom equal to the residual d.f. (n-p):

```{r}
1-pchisq(fit2$deviance, df = fit2$df.residual)
```

So, this p value is 0, which means the model does not fit the data.


###Modeling Overdispersion via a Quasi distribution
For the Poisson , we can fit a "quasi" distribution that adds an extra parameter to allow the mean-variance relationship to not be constant. 
For Poisson we get:

$$var(Y) = \lambda * \phi$$, instead of $$var(Y) = \lambda $$

This allows us to include a rough proxy for a dispersion parameter for the distribution. Naturally this is fixed at 1 for basic models, and estimated in the quasi models, we can look to see if is much bigger than 1. If overdispersion is present and not accounted for you could identify a relationship as being significant when it is not!

```{r}
fit3<-glm(healthdays~factor(race_eth)+factor(educ)+factor(agec), data=brfss16m, family=quasipoisson)
summary(fit3)
```

##Other count models - Negative binomial
* Of course, we could just fit other distributional models to our data, popular choices are:

* Negative binomial
-   Effectively adds a shape parameter to Poisson 

$$Y \sim NB (\lambda, \lambda+\lambda^2/\theta),\text{   } E(Y) = \lambda,\text{   } var(Y) = \lambda+\lambda^2/\theta$$
$$\lambda = log(\eta)$$
$$\eta = \beta_0 + \beta_1 x_1+ log(n)$$

Now, R will not fit negative binomial models using survey design, so, we will fit them using sample weights only, then calculate the robust standard errors. We standardize the weights to equal the sample size, as opposed to the population size by dividing each person's weight by the mean weight.

```{r}
#First, I define a function to get the clustered, or robust standard errors.
#This function effectively controls for the within-strata homogeneity when
#calculateing the se's for the betas. 

#I stole this from: http://drewdimmery.com/robust-ses-in-r/
#and http://people.su.se/~ma/clustering.pdf
#I also added a correction to use this with the hurdle and zero-inflated models
#This is how stata gets robust se's

clx2 <-   function(fm, dfcw,  cluster){
    # R-codes (www.r-project.org) for computing
    # clustered-standard errors. Mahmood Arai, Jan 26, 2008.
    
    # The arguments of the function are:
    # fitted model, cluster1
    # You need to install libraries `sandwich' and `lmtest'
    
    # reweighting the var-cov matrix for the within model
    require(sandwich);require(lmtest)
    if(class(fm)=="zeroinfl"|class(fm)=="hurdle") {
    M <- length(unique(cluster))   
    N <- length(cluster)           
    K <- dim(fm$vcov)[1]        #here is the rank from the zero inflated fits             
    dfc <- (M/(M-1))*((N-1)/(N-K))  
    uj  <- apply(estfun(fm),2, function(x) tapply(x, cluster, sum, na.rm=T));
    vcovCL <- dfc[1]*sandwich(fm, meat=crossprod(uj)/N)*dfcw #fix a length problem in dfc
    list(summary=coeftest(fm, vcovCL))}
    else if(class(fm)!="zeroinfl"){
    M <- length(unique(cluster))
    N <- length(cluster)
    K <- fm$rank
    dfc <- (M/(M - 1)) * ((N - 1)/(N - K))
    uj <- apply(estfun(fm), 2, function(x) tapply(x, cluster, sum, na.rm=T));
    rcse.cov <- dfc * sandwich(fm, meat = crossprod(uj)/N)
    rcse.se <- coeftest(fm, rcse.cov)
    return(list( rcse.se))}
}
```

Now, i'll illustrate that this works:

```{r}
#Fit poisson, and compare it to the fit from the survey design
#Fit the Poisson GLM
sub$wts<-sub$mmsawt/mean(sub$mmsawt, na.rm=T)
fit.pois<-glm(healthdays~factor(race_eth)+factor(educ)+factor(agec),
              data=sub,
              weights=wts, family=poisson)

library(lmtest)
library(sandwich)
clx2(fit.pois,cluster =sub$ststr)
coeftest(fit.pois, vcov=vcovHC(fit.pois, type="HC1",cluster="ststr"))
#same as survey!

summary(fit1)

#Fit the Negative Binomial GLM
library(MASS)
fit.nb2<-glm.nb(healthdays~factor(race_eth)+factor(educ)+factor(agec),
              data=sub,
              weights=wts)
clx2(fit.nb2,cluster =sub$ststr)

coeftest(fit.nb2, vcov=vcovHC(fit.nb2, type="HC1",cluster="ststr"))



```


##Aggregate data
Typically when we get data aggregate data, they have a spatial component, meaning they are aggregated to some geography
 
  * Tract, block, county, ZIP code, etc.
 
  * These are typically either raw counts
  
      + Count of deaths, crimes, births, people with some characteristic (eg poverty) > numerator
  
  * Hopefully with a population at risk > denominator, or rates that someone has calculated
   
    + Mortality rates from NCHS or some census rate

GLMs are a class of statistical models that link the mean of the distribution to a linear combination of covariates by some form of link function.

$$g(u) = g(E(Y)) = X ' \beta$$

Where g() is the link function. The link function for a binomial distribution is typically the logit, or:

$$g(u) = log(\frac{\pi}{1- \pi})$$ 

##Modeling options
* Inherently, we want to see what influences the rate in our areas

    + This is for aggregate data
    
* So we want regression models for our area-based counts that properly model the independent variable

* You have a few options: Binomial, Poisson, Normal, Negative binomial

```{r, results='hide'}
#I subset to only US counties this time, for the sake of computational time.
library(dplyr)
library(tidycensus)
library(acs)
library(lmtest)
library(sandwich)
githubURL <- "https://github.com/coreysparks/data/blob/master/cmf06_10_age_sex_std.Rdata?raw=true"
load(url(githubURL))

dim(cmf)
githubURL <- "https://github.com/coreysparks/data/blob/master/arf1516.Rdata?raw=true"
load(url(githubURL))

dim(arf15)

```
```{r, echo=F}

census_api_key("997fb9115102b709d5028501b4b030e84af62525")
```
```{r}

#Gini coefficent for income inquality
gini<-get_acs(geography = "county", variables = "B19083_001", year = 2009 )


usdata<-merge(x = arf15, y=cmf, by.x="f00004", by.y="County.Code")
library(tigris)
```

```{r, echo=FALSE, results=FALSE}
usco<-counties(state = "*",  year = 2010,cb = T )
```

```{r, eval=FALSE}
usco<-counties(state = "*",  year = 2010,cb = T )
```

```{r}
library(sf)
library(ggplot2) #note you must do: devtools::install_github("tidyverse/ggplot2") to get geom_sf() geometries
```

```{r}
usco<- st_as_sf(usco)
usco<-mutate(usco, geoid=paste(STATEFP, COUNTYFP, sep=""))
usco<-inner_join(usco, y=usdata, by=c("geoid"="f00004") ) 
usco<-inner_join(usco,y=gini, c("geoid"="GEOID") )

usco<-mutate(usco,pblack=100*( (f1391005 + f1391105)/ f1198405  ),
             phisp=100*( (f1392005+ f1392105)/ f1198405 ),
             prural = 100*( f1367900/ f0453000 ),
             pfemhh =  f0874600 ,
             unemp= f0679505,
             medhouseval=f1461306,
             popdensity=f1198405/(as.numeric(CENSUSAREA)/1000000),
             gini=estimate )
usco<-  filter(usco, !STATEFP %in% c("02", "15", "72"), is.na(Age.Adjusted.Rate)==F&is.na(pblack)==F&is.na(prural)==F&is.na(pfemhh)==F) 

usco<-  mutate( usco, 
                mortquant = cut( Age.Adjusted.Rate, breaks= quantile(Age.Adjusted.Rate, p=seq(0, 1, .2), na.rm=T)),
                pblackq=cut( pblack, breaks= quantile(pblack, p=seq(0, 1, .2), na.rm=T)), 
                giniq=cut(gini, breaks=quantile(gini, p=seq(0,1,.2), na.rm=T)))

usco %>%
  ggplot()+geom_sf(aes( fill=mortquant))+coord_sf(crs = 102009)+ scale_colour_brewer(palette = "Blues" )+scale_fill_brewer(palette = "Blues", na.value="grey")+guides(fill=guide_legend(title="Mortality Quartile"))

usco %>%
  ggplot()+geom_sf(aes( fill=pblackq))+coord_sf(crs = 102009)+ scale_colour_brewer(palette = "Blues" )+scale_fill_brewer(palette = "Blues", na.value="grey")+guides(fill=guide_legend(title="Percent Black Population Quartile"))

usco %>%
  ggplot()+geom_sf(aes( fill=giniq))+coord_sf(crs = 102009)+ scale_colour_brewer(palette = "Blues" )+scale_fill_brewer(palette = "Blues", na.value="grey")+guides(fill=guide_legend(title="Gini Coefficient Quartile"))

```


###Binomial model for counts
To use the Binomial model, you have two options
Your observations are 1/0, we have done this before. This says that each area has the event or not OR Your observations are a combination of y and n, where y is a count of some event which has happened to some portion of the population at risk, n. For either of these, you are modeling the probability of observing an event, $\pi$  or really the logit of the probability: $$log(\frac{\pi}{1- \pi}) = X' \beta$$
*  The good thing is that you have already done this!
* All interpretation of parameters is the same as logistic regression 

$$exp(\beta)$$

*  All model testing is the same too!

```{r,fig.width=7, fig.height=8}
usfitbin<-glm(cbind(Deaths, Population)~scale(pblack)+scale(phisp)+scale(prural)+scale(pfemhh)+scale(unemp)+scale(medhouseval)+scale(popdensity)+scale(gini) ,family=binomial,
            data=usco)

summary(usfitbin)
round(exp(coef(usfitbin)), 3)

coeftest(usfitbin, vcov.=vcovHC(usfitbin, type="HC1"))

1-usfitbin$deviance/usfitbin$null.deviance

#This should be the crude death rate, per 1,000 persons
1000 * mean(fitted(usfitbin))
hist(1000*fitted(usfitbin), main="US Crude Death Rate Distribution from Binomial Model")

```

###Poisson model for counts
For the Poisson model, you observe some count of events in each area (crimes, deaths, etc), and you also have some population at risk in each area. This is generally not the same for each area, but could be, you want a Poisson rate $(\lambda)$, which is really the average count (y). We have several ways of modeling the Poisson count: 

_Pure count_ If each area has the same risk set, _Rate_, include an offset term in your model to
incorporate unequal risk 

$$log(y)= X' \beta + log(n)$$

, where n is the population at risk in each area. This is called the _offset_ term in the model. _Standardized ratio_ incorporate differential exposure as an expected count 

$$log(y)= X' \beta + log(E)$$.

Again, all interpretation of parameters is done on a log scale, so 

$$exp(\beta) = \text{ % change in the mean}$$
, or % change in the rate, for a 1 unit change in X. All testing is done in the same manner.

```{r,fig.width=7, fig.height=8}
#Form the expected count
usco$E<-usco$Population * (sum(usco$Deaths)/sum(usco$Population))
fit.poi.us<-glm(Deaths~offset(log(E))+scale(pblack)+scale(phisp)+scale(prural)+scale(pfemhh)+scale(unemp)+scale(medhouseval)+scale(popdensity)+scale(gini) ,family=poisson,
            data=usco)
summary(fit.poi.us)
lmtest::coeftest(fit.poi.us, vcov.=vcovHC(fit.poi.us, type="HC1"))
1-fit.poi.us$deviance/fit.poi.us$null.deviance

scale<-sqrt(fit.poi.us$deviance/fit.poi.us$df.residual)
scale


1-pchisq(fit.poi.us$deviance, df = fit.poi.us$df.residual)

round(exp(coef(fit.poi.us)), 3)


hist(fitted(fit.poi.us)/usco$E,main="US SMR Distribution from Poisson Model" )
```



###Other 2 - parameter distributions
* Of course, we could just fit other distributional models to our data, popular choices are:
* Normal
- y has mean and variance
- Effectively adds a shape parameter to Poisson 


```{r,fig.width=7, fig.height=8}
library(MASS)
fit.nb.us<-glm.nb(Deaths~offset(log(E))+scale(pblack)+scale(phisp)+scale(prural)+scale(pfemhh)+scale(unemp)+scale(medhouseval)+scale(popdensity)+scale(gini),
            data=usco)
summary(fit.nb.us)
 1-fit.nb.us$deviance/fit.nb.us$null.deviance

lmtest::coeftest(fit.nb.us, vcov.=vcovHC(fit.nb.us, type="HC1"))

round(exp(coef(fit.nb.us)), 3)
hist(fitted(fit.nb.us)/usco$E,main="US SMR Distribution from Negative Binomial Model" )


```
