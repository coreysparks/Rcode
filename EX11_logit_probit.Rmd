---
title: "DEM 7273 -  Logit Models Part 2"
author: "Corey Sparks, PhD"
date: "November 29, 2017"
output:
  html_document:
    fig_height: 7
  pdf_document:
    latex_engine: xelatex
---


Last time, we saw the logistic regression model:

If our outcome is dichtomous (1/0), the natural distribution to consider for a GLM is the binomial
$$y \sim \ \text{Binomial}\binom{n}{p}$$
with $p$ being the mean of the binomial, and n being the number of trials, generally when you have individual data, n is always 1, and p is the probability of observing the 1, conditional on the observed predictors. There are two common techniques to estimate the mean, logistic and probit regression. In a Logistic model, the link function is the inverse logit function, or

$\text{Logit}^{-1}(p) =log \frac{p}{(1-p)}$

Which gives us the following conditional mean model:

$$E(y|x)  = \frac{1}{1+ exp({-\sum_k \beta_k x_{ik}})}$$
Which situates the model within the logistic distribution function. Expressing *p* as a linear model is done via this log odds transformation of the probability:

$$log \frac{p}{(1-p)} = \sum_k \beta_k x_{ik}$$

##Stratified models
Often in the literature, we will see models stratified by some predictor. This is usually because a specific hypothesis is stated regarding how the effect of a particular predictor varies by some categoricial variable. In this case, we may be interested in considering if education or smoking universally affects the poor health outcome. We get at this by *stratifying* our analysis by race (in this example).

The easiest way to do this is to subset the data by race and run the models separately. 

**The first thing we do** is test for the interaction of education and race. If this interaction is not significant, we have no justification for proceeding, becuase the effect of education does not vary by race group. **This is the test of parallel slopes, a' la the ANCOVA model**

```{r}
fit.logitint<-svyglm(badhealth~race_eth*educ+ins+smoke,design= des, family=binomial)#race*education interaction+health behaviors
regTermTest(fit.logitint, test.terms = ~race_eth:educ, method = "Wald", df=NULL)
```


```{r}
library(haven)
library(dplyr)
library(stargazer)
library(ggplot2)
library(knitr)

ipums<-read_dta("https://github.com/coreysparks/data/blob/master/usa_00045.dta?raw=true")

newpums<-ipums%>%
  filter(labforce==2, age>=18, incwage>0, relate==1)%>%
  mutate(mywage= ifelse(incwage%in%c(999998,999999), NA,incwage),
         unemp=ifelse(empstat==2,1,0), #unemployment
         mig=as.factor(migrate1), #moved in past year
         sexrecode=ifelse(sex==1, "male", "female"))%>%
  mutate(race_eth = case_when(.$hispan %in% c(1:4) & .$race %in%c(1:9) ~ "hispanic", 
                          .$hispan ==0 & .$race==1 ~"0nh_white",
                         .$hispan ==0 & .$race==2 ~"nh_black",
                         .$hispan ==0 & .$race%in%c(3,7,8,9) ~"nh_other",
                         .$hispan ==0 & .$race%in%c(4:6) ~"nh_asian",
                          .$hispan==9 ~ "missing"))%>%
  mutate(edurec = as.factor(case_when(.$educd %in% c(0:61)~"nohs", 
                            .$educd %in% c(62:64)~"hs",
                            .$educd %in% c(65:100)~"somecoll",
                            .$educd %in% c(101:116)~"collgrad", 
                            .$educd ==999 ~ "missing")) )

test<-model.matrix(~newpums$edurec-1)
test<-model.matrix(~edurec-1 + race_eth+unemp+mywage+age+I(age^2), contrasts.arg = list(edurec ="contr.treatment", race_eth ="contr.treatment"), data=newpums)
ss<-numeric()
for(i in 1:20){
  ss[i]<-kmeans(test, centers=i, nstart = 10)$tot.withinss
}


plot(x=1:20, y=ss, type="l")

test2<-kmeans(test, centers = 5)
test3<-hclust(d = dist(test[sample(1:dim(test)[1], size = 10000),]), method = "ward.D2")

library(randomForest)
n<-dim(test)[1]
test4<-randomForest(y=newpums$mig[1:10000], test[1:10000,],ytest=newpums$mig[10001:11000], xtest = test[10001:11000,], nPerm = 10, strata = factor(newpums$statefip))

test4
```



For the Probit model, the link function is the inverse cumulative Normal distribution:

$$E(y|x) = \Phi^{-1}(p) = \Phi (\sum_k \beta_k x_{ik})$$

In practice, these two models give very similar estimates and will have very similar interpretations, although the logitistic regression model has the more convenient odds ratio interpretation of its $\beta's$, while the probit model's coefficients are often transformed into marginal coefficients, which is more of a challenge and software generally doesn't give you these by default. 


##Logit/Probit Regression example
There is no trick to fitting logistic regression models, just use the `glm()` function with the apppriate distribution specified via `family=binomial` for logistic and `family=binomial(link="probit")` for the probit model. You don't have to specify the link function if you're just doing the logistic model, as it is the default. 


```{r}
#Logit model
fit.logit<-glm(mig~race_eth+edurec+sexrecode+scale(age)+scale(I(age^2)),data=newpums, family=binomial)

summary(fit.logit)

```

And the probit model:

```{r}
#probit model
fit.probit<-glm(mig~race_eth+edurec+sexrecode+scale(age)+scale(I(age^2)),data=newpums, family=binomial(link= "probit"))

summary(fit.probit)

```

Both of these models show the exact same patterns of effects, with Hispanics, blacks and multi-race individuals showing increased chances of reporting poor/fair health, when compared to whites (Reference group). Similarly, the education variables shows a negative linear trend, with those with more education having lower chances of reporting poor/fair health compared to those with a primary school education (Reference group), and likewise, as people get older, they are more likely to report poor/fair health, compared to those under age 24 (Reference group).

#Present both model coefficients next to one another

```{r , results='asis'}
stargazer(fit.logit, fit.probit, type = "html", style = "demography", ci = T)
```




##Fitted Values
As I often say, I like to talk about "interesting cases". In order to do this, you need the fitted mean for a particular case. This is done by getting the fitted values for that case from the model. To do this, I generate a bunch of "fake people" that have variability in the model covariates, and fit the model for each type of person. This is perhaps overkill in this example because I fit every type of person, ideally you would want a few interesting cases to discuss.

In order to derive these, we effectively "solve the equation" for the model, or another way of saying it, we estimate the conditional mean of y, by specifying the x values that are meaningful for a particular comparison.
For example the probabilty of a white, young college educated person reporting poor health is just the estimate of the model, evaluated at those particular characteristics:

$$\text{Pr(poor/fair health)} =  \frac{1}{1+exp({\beta_0 + \beta_1*white + \beta_2*young+\beta_3*college})}$$


```{r}
#get a series of predicted probabilites for different "types" of people for each model
#expand.grid will generate all possible combinations of values you specify
dat<-expand.grid(race_eth=levels(factor(newpums$race_eth)), edurec=levels(factor(newpums$edurec)), age=seq(25, 80 ,5), sexrecode=levels(factor(newpums$sexrecode)) )

#You MAY need to get rid of impossible cases here

#generate the fitted values
fit<-predict(fit.logit, newdat=dat,type="response")
fitp<-predict(fit.probit, newdat=dat,type="response")
#add the values to the fake data
dat$fitted.prob.lrm<-round(fit, 3)
dat$fitted.prob.pro<-round(fitp, 3)

#Print the fitted probabilities for the first 20 cases
head(dat, n=20)

```
Which shows us the estimated probabilty of reporting moving in the last year for each specified type of "fake person" that we generate. For example, let's look at the probability for a Non-Hispanic white, age 25 with a college education, compared to a Hispanic person, age 25 with a high school education:

```{r}
dat[which(dat$race_eth=="0nh_white"&dat$age==25&dat$edurec=="collgrad"),]

dat[which(dat$race_eth=="hispanic"&dat$age==25&dat$edurec=="hs"),]


```
The first case has an estimated probability of reporting poor/fair health of about 39% form females and 41% for males, while the second case has a 35% chance for females and a 36% chance for males. These are often more effective ways to convey the result of a model, instead of talking about all the regression coefficients. 

##Marginal effects 
In a regression model in general, the $\beta's$ are the solution to the differential equation:
$$\frac{\partial y}{\partial x} = \beta$$

which is just the rate of change in y, given x, known as the marginal effect. This is the case for *strictly linear model*

In the logit and probit model, which are nonlinear models, owing to their presumed model structure, the marginal effect also has to take into account the change in the respective pdf with respect to the mean, or:

$$\frac{\partial y}{\partial x} = \beta *\frac{\partial \Phi(x' \beta)}{\partial x'\beta}$$

So we have to multiply the estimated $\beta$ by the p.d.f. of the assumed marginal distribution evaluated at the mean function. In R that's not big problem:

```{r}
#Logit marginal effects
log.marg<-coef(fit.logit)*mean(dlogis(predict(fit.logit)), na.rm=T)

#for probit now
prob.marg<-coef(fit.probit)*mean(dnorm(predict(fit.probit)), na.rm=T)

plot(log.marg[-1], ylab="Marginal Effects", axes=T,xaxt="n", main="Marginal Effects from Logit and Probit models", ylim=c(-.3, .3))
axis(side=1, at=1:10, labels=F)
text(x=1:10, y=-.4,  srt = 45, pos = 1, xpd = TRUE,
     labels = c( "Hispanic", "NH asian","NH black" ,"NH other","HS",
                 "NO HS","some coll", "male", "age", "age^2" ))
points(prob.marg[-1], col=2)
abline(h=0, col=3)
legend("topleft", legend=c("Logit Model", "Probit Model"), col=c("black", "red"),pch=1)
```

Which shows us that the marginal effects are very similar between the two models. We can coax these into a table like:

```{r}
data.frame(m.logit=log.marg, m.probit=prob.marg)
```
