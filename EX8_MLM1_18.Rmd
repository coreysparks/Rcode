---
title: "DEM 7283 - Multi-level Models Example 1"
author: "Corey Sparks, PhD"
date: "March 19, 2018"
output:
  html_document:
    toc: yes
  pdf_document:
    latex_engine: xelatex
    toc: yes
  word_document:
    keep_md: yes
    toc: yes
header-includes: \usepackage{bbm}
---

```{r "setup", include=FALSE, echo=F}
require("knitr")
opts_knit$set(root.dir = "C:/Users/ozd504/Google Drive/classes/dem7283/class18/data/")
```
#Lecture notes

Fundamental problem
========================================================
**Names**
- Mixed effects models
- Hierarchical linear models
- Random effects models

These all refer to the same suite of models, but different disciplines/authors/applications have managed to confuse us all.

Perspectives in Multi-level Modeling
========================================================

**Basic multilevel perspectives**
* The Social epidemiology perspective
* General ecological models
* Longitudinal models for change

Each of these situate humans within some higher, contextual level that is assumed to either influence or be influenced by their actions/behaviors

And we would ideally like to be able to incorporate covariates at the individual and higher level that could influence behaviors

Multi-stage sampling
========================================================
* Non-random sampling
* Population consists of known sub-groups called *clusters*
* A 2 -stage sample might be households within neighborhoods, or children within schools
  + We may choose a random sample of schools/neighborhoods at the first stage, and a random sample of people within each school/neighborhood as the second stage
  + We need to be *careful* because the observations in the second stage are not *independent* of one another
  + Increased probability of selection for children in a selected school
* This type of sampling leads to *dependent* observations 

*Figures from [Kawachi and Berkman (2003)](https://global.oup.com/academic/product/neighborhoods-and-health-9780195138382?cc=us&lang=en&)*
 

Here's a picture of this:

 ![Multistage Sampling](C:/Users/ozd504/Google Drive/classes/dem7903_App_Hier/Rpresentations/multistage.png)

Multi-level propositions
========================================================
When we have a research statement that involves individuals within some context, this is a multi-level proposition. In this sense, we are interested in questions that relate variables at different levels, the micro and the macro. This also holds in general if a sample was collected with a multi-stage sampling scheme. 

In a multilevel proposition, variables are present at two different levels, and we are interested in the relationship between both the micro and macro level association with our outcome, y.

 ![Multi-level](C:/Users/ozd504/Google Drive/classes/dem7903_App_Hier/Rpresentations/fig2_2.png)

This can be contrasted with a purely micro level proposition, where all our observed variables are the level of the individual

 ![micro-level](C:/Users/ozd504/Google Drive/classes/dem7903_App_Hier/Rpresentations/fig2_3.png)
 
Likewise, if we are only interested in the relationship between macro level variables, we have this situation:

 ![macro-level](C:/Users/ozd504/Google Drive/classes/dem7903_App_Hier/Rpresentations/fig2_4.png)
 
**Macro - micro propositions**

We commonly encounter the situation where a macro level variable effects a micro level outcome. This can happen in several different ways.

 ![macro-micro](C:/Users/ozd504/Google Drive/classes/dem7903_App_Hier/Rpresentations/fig2_5.png)

The first case is a macro to micro proposition, which may be exemplified by a statement such as: "Individuals in areas with high environmental contamination leads to higher risk of death".

Whereas the second frame illustrates a more specific special case, where there is a macro level effect, net of the individual level predictor, and may be stated "For individuals with the a given level of education, living in areas with high environmental contamination leads to higher risk of death". 

The last panel illustrates what is known as a *cross level interaction*, or a macro-micro interaction. This is where the relationship between x and y is dependent on Z. This leads to the statement "Individuals with low levels of education, living in areas with high environmental contamination have higher risk of death".


Longitudinal Models
========================================================

These kinds of models are used when you have observations on the same individuals over multiple time points
* Don't have to be the same times/ages for each observation
    + More flexibility
* These models treat the individual as the higher level unit, and you are interested in studying
* Change over time within an individual
* Impacts of prior circumstances on later outcomes
* Two modeling strategies will allow us to consider individual change over time, versus population averaged change over time.

Multilevel data Preface
========================================================
* Not all data sets will allow you to do multi-level modeling
    + Many data sets don't have any higher level units identified, or the units they do have are not necessarily meaningful
* Not all problems are multi-level problems
    + Unless you are specifying a problem that is interested in how some characteristic of some higher level structure is influencing behavior, these models are not for you.

Linear Mixed Model
========================================================
* In the traditional linear model, groups are treated as fixed effects
  + ANOVA, ANCOVA, MANOVA
* For instance the ANOVA model assumes that the group effects are fixed and do not change relative to the reference group
  + Also the groups represent distinct representations of all possible groups in the population
* This model is of the form:

$$y_{ij} = \mu + u_{j} + e_{ij}$$

where $$\mu$$ is the grand mean, $u_{j}$ is the fixed effect of the $j^{th}$ group and $e_{ij}$ is the residual for each individual

========================================================
This model assumes that you are capturing all variation in y by the group factor differences, $u_{j}$  alone. 

**If** you have all your groups, 

**and** your only predictor is the group (factor) level,

**and** if you expect there to be directional differences across groups a priori,

*then* this is probably the model for you.

You might use this framework if you want to crudely model the effect of region of residence in an analysis.
+ i.e. is the mean different across my region?


ANOVA and ANCOVA
========================================================
* The ANOVA and ANCOVA models are extremely useful if:
 + You simply want to test differences in the mean across groups *(ANOVA)*
 
 $$ y_{ij} = \mu + u_{j} + e_{ij} $$
 
  + Each cell (group) has its mean defined by:
  
 $$ \mu + u_{j} $$

  + And we *typically* set one group as the "reference group"
  + We test if each $$\mu_{j} = 0$$ using a t-test and see if all our group means are equal
  + Also, the global F-test will show us if *ANY* of the means are different from one another
    
* In the ANCOVA model, you want to examine the effect of a covariate in each group 
*(ANCOVA)*

$$ y_{ij} = \mu + u_{j}*\beta x_{i}+ e_{ij} $$
  + as a simple example

  + This model contains the interaction between the group factor $u_{j}$ and $x_i$
  + This is often called the *parallel slopes* model, because it is testing the assumption
from the simpler model:

  $$ y_{ij} = \mu + u_{j}+\beta x_{i}+ e_{ij} $$
  
  + That all groups have the same $\beta$ effect on the mean

Basic Random Effect Models
========================================================
* Consider the ANOVA model:

   $$ y_{ij} = \mu + u_{j} + e_{ij} $$
   
  + The random effects model assumes that each of the group means $\mu + u_j$ are composed of a grand mean and an *iid* random effect

  + This differs from the ANOVA model because the $u_j$'s are not considered *fixed*, by setting a comparison group. 

   $$ y_{ij} = \mu + u_{j} + e_{ij} $$ 
   
+ Generally, this *iid* random effect, *u* is assumed to come from :

  $$u_j \sim N(0, \sigma^2)$$  
  
  + the random effects are centered around the mean $\mu$
  + So that's why there's a 0 mean, and the variation in the groups is modeled by the estimated variance in the distribution  $\sigma^2$
   +Basically, if $\sigma^2$ = 0, then there is no variation between groups!  
* This model is called the random intercept model, because only the 
*intercepts* are allowed to vary randomly

Choosing...
========================================================
* There are differences between these classic models and the linear mixed model.  As a rule, you use the fixed-effects models when:

  + 1) You know that each group is regarded as unique, and you want to draw conclusions on each of these specific groups, 
  + and you also know all the groups a priori e.g. sex or race

+ 2) If the groups can be considered as a sample from some (real or hypothetical) population of groups, and you want to draw conclusions about this population of groups, then the random effects model is appropriate.
  + *WHY?*  because if you have a *LARGE* number of groups, 
  + say $n_j$ > 10, then the odds that you are really interested in all possible difference in the means is probably pretty low

Forms of the random effect model
========================================================
* There are 2 
*basic* forms for the mixed model
* These models may be extended in MANY, MANY, MANY more ways
  + *which is why we're here*
  + Random Intercepts model
  + Random Slopes model

Random Intercept Model
========================================================
* The random intercept model assumes you have:
  + *j groups (j=1 to J)*
  + *i individuals within the j groups (i=1 to $n_j$)*
  + for each individual in the *j* groups you have measured $y_{ij}$ and $x_{ij}$ 
and *potentially* for each group *j*, we have may have measured $z_j$ which is a covariate measured at the group level
For example, you do a survey on health and you measure:
+ y = the health status of each individual
+ x= SES, race, etc of each individual
+ j = the county each individual lives in, and
+ z = the poverty rate or median income in the county


* We write our full model, with 
*k* predictors as:
$$y_{ij} = \beta_{0j} + \sum_{k} {\beta_k x_{ik}} + \gamma z_j + e_{ij}$$
  + This model has a few features that we can use or not use, as it suits us
  + e.g. if we don't have a group-level predictor *z*, then we won't have that component of the model
  + $\beta_{0j}$ is called the random intercept, 
  + We can write the random intercept as:
   $$ \beta_{0j} = \beta_0 + u_{j} $$
  + i.e. a fixed mean intercept and each group's *iid* deviation from it
  + and again *u* is assumed to come from :
  $$u_j \sim N(0, \sigma^2)$$ 
  

* Graphically the $\beta_{0j}$ term can be seen as:

 ![Random Intercepts](C:/Users/ozd504/Google Drive/classes/dem7903_App_Hier/Rpresentations/randintercept.png)


Variance components
========================================================
* This model also estimates variance components, so you can see how much variability is accounted for by adding the random intercept term.
  + if var( $y_{ij}$) is the total variance, $\sigma^2$
  + and var($u_j$) is the higher level variance in the random intercepts, $\sigma^2 _{u}$
  + and var($e_{ij}$) is the residual individual level variance, $\sigma^2 _{e}$
+ we can write the total variance as:

$\sigma^2$ = $\sigma^2 _{e}+\sigma^2 _{u}$
  + These are called the variance components of the model, 
  + and separate the variance into differences between individuals and differences between groups

  + the correlation between any two individuals within a given group is:
  
   $$\rho(y_{ij},y_{i'j}) = \frac{ \sigma^2 _{u} }{ \sigma^2 _{u} + \sigma^2 _{e} }$$
  
  + is called the *intra-class correlation coefficient*, and can be interpreted as the correlation between 2 random individuals in a random group, but, I find it more informative to interpret as the fraction of the variance that is due to the groups.



#Empirical Example
In this example, I introduce how to fit the multi-level model using the `lme4` [package](http://cran.r-project.org/web/packages/lme4/index.html). This example considers the linear case of the model, where the outcome is assumed to be continuous, and the model error term is assumed to be Gaussian. Subsequent examples will highlight the Generalized Linear Mixed Model (GLMM). 

This example shows how to:
* Examine variation between groups using fixed effects
* Fit the basic random intercept model : 

$y_{ij} = \mu + u_{j} + e_{ij}$ with $u_j \sim N(0, \sigma^2)$

Where the intercepts ($u_j$) for each group vary randomly around the overall mean ($\mu$)

*I also illustrate how to include group-level covariates and how to fit the random slopes model and the model for cross level interactions

The example merges data from the 2014 CDC Behavioral Risk Factor Surveillance System (BRFSS) SMART MSA data. [Link](https://www.cdc.gov/brfss/smart/smart_2016.html) and the 2010 American Community Survey 5-year estimates at the MSA level. More details on these data are found below.

```{r load data&recode, message=FALSE, warning=FALSE}
#load brfss
library(car)
library(stargazer)
library(survey)
library(sjPlot)
library(ggplot2)
library(pander)
library(knitr)
load("~/Google Drive/classes/dem7283/class18/data/brfss16_mmsa.Rdata")
set.seed(12345)
#samps<-sample(1:nrow(brfss16m), size = 40000, replace=F)
#brfss16m<-brfss16m[samps,]
#The names in the data are very ugly, so I make them less ugly
nams<-names(brfss16m)
#we see some names are lower case, some are upper and some have a little _ in the first position. This is a nightmare.
newnames<-gsub(pattern = "_",replacement =  "",x =  nams)
names(brfss16m)<-tolower(newnames)

```

###Recode variables
```{r}
#sex
brfss16m$male<-ifelse(brfss16m$sex==1, 1, 0)

#BMI
brfss16m$bmi<-ifelse(is.na(brfss16m$bmi5)==T, NA, brfss16m$bmi5/100)

#Healthy days
brfss16m$healthdays<-recode(brfss16m$physhlth, recodes = "88=0; 77=NA; 99=NA")

#Healthy mental health days
brfss16m$healthmdays<-recode(brfss16m$menthlth, recodes = "88=0; 77=NA; 99=NA")

brfss16m$badhealth<-recode(brfss16m$genhlth, recodes="4:5=1; 1:3=0; else=NA")
#race/ethnicity
brfss16m$black<-recode(brfss16m$racegr3, recodes="2=1; 9=NA; else=0")
brfss16m$white<-recode(brfss16m$racegr3, recodes="1=1; 9=NA; else=0")
brfss16m$other<-recode(brfss16m$racegr3, recodes="3:4=1; 9=NA; else=0")
brfss16m$hispanic<-recode(brfss16m$racegr3, recodes="5=1; 9=NA; else=0")

brfss16m$race_eth<-recode(brfss16m$racegr3, 
recodes="1='nhwhite'; 2='nh black'; 3='nh other';4='nh multirace'; 5='hispanic'; else=NA",
as.factor.result = T)
brfss16m$race_eth<-relevel(brfss16m$race_eth, ref = "nhwhite")

#insurance
brfss16m$ins<-recode(brfss16m$hlthpln1, recodes ="7:9=NA; 1=1;2=0")

#income grouping
brfss16m$inc<-recode(brfss16m$incomg, recodes = "9= NA;1='1_lt15k'; 2='2_15-25k';3='3_25-35k';4='4_35-50k';5='5_50kplus'", as.factor.result = T)
brfss16m$inc<-as.ordered(brfss16m$inc)
#education level
brfss16m$educ<-recode(brfss16m$educa,
recodes="1:2='0Prim'; 3='1somehs'; 4='2hsgrad'; 5='3somecol'; 6='4colgrad';9=NA",
as.factor.result=T)
brfss16m$educ<-relevel(brfss16m$educ, ref='2hsgrad')

#employloyment
brfss16m$employ<-recode(brfss16m$employ1,
recodes="1:2='employloyed'; 2:6='nilf'; 7='retired'; 8='unable'; else=NA",
as.factor.result=T)
brfss16m$employ<-relevel(brfss16m$employ, ref='employloyed')

#marital status
brfss16m$marst<-recode(brfss16m$marital,
recodes="1='married'; 2='divorced'; 3='widowed'; 4='separated'; 5='nm';6='cohab'; else=NA",
as.factor.result=T)
brfss16m$marst<-relevel(brfss16m$marst, ref='married')

#Age cut into intervals
brfss16m$agec<-cut(brfss16m$age80, breaks=c(0,24,39,59,79,99))

#BMI, in the brfss16ma the bmi variable has 2 implied decimal places,
#so we must divide by 100 to get real bmi's

brfss16m$bmi<-brfss16m$bmi5/100

#smoking currently
brfss16m$smoke<-recode(brfss16m$smoker3, 
recodes="1:2=1; 3:4=0; else=NA")
#brfss16m$smoke<-relevel(brfss16m$smoke, ref = "NeverSmoked")

brfss16m$obese<-ifelse(is.na(brfss16m$bmi)==T, NA, 
                       ifelse(brfss16m$bmi>30,1,0))

```

I want to see how many people we have in each MSA in the data:

```{r}

#Now we will begin fitting the multilevel regression model with the msa
#that the person lives in being the higher level
head(data.frame(name=table(brfss16m$mmsaname),id=unique(brfss16m$mmsa)))
#people within each msa

#How many total MSAs are in the data?
length(table(brfss16m$mmsa))
#MSAs
```


###Higher level predictors
We will often be interested in factors at both the individual *AND* contextual levels. To illustrate this, I will use data from the American Community Survey measured at the MSA level. Specifically, I use the DP3 table, which provides economic characteristics of places, from the 2010 5 year ACS [Link](http://www.census.gov/acs/www/data_documentation/special_data_release/).
```{r, echo=FALSE}
mykey<-"997fb9115102b709d5028501b4b030e84af62525"
```


```{r load_acs,message=FALSE, warning=FALSE}
library(acs)
#Get 2010 ACS median household incomes for tracts in Texas
msaacs<-geo.make(msa="*")

#ACS tables B17 is poverty, B19 is Gini, B25001 is housing vacancy, B25035 is median year built 
acsecon<-acs.fetch(key=mykey, endyear=2010, span=5, geography=msaacs, variable = c("B19083_001","B17001_001","B17001_002" , "B25002_001","B25002_003", "B25035_001"))

colnames(acsecon@estimate)

msaecon<-data.frame(gini=acsecon@estimate[, "B19083_001"], 
ppoverty=acsecon@estimate[, "B17001_002"]/acsecon@estimate[, "B17001_001"],
giniz=scale(acsecon@estimate[, "B19083_001"]),
pvacant=acsecon@estimate[,"B25002_003"]/acsecon@estimate[, "B25002_001"],
ppovertyz=scale(acsecon@estimate[, "B17001_002"]/acsecon@estimate[, "B17001_001"]), 
pvacantz=scale(acsecon@estimate[,"B25002_003"]/acsecon@estimate[, "B25002_001"]), 
medhouse=acsecon@estimate[, "B25035_001" ],
medhousez=scale(acsecon@estimate[, "B25035_001" ]))

msaecon$ids<-paste(acsecon@geography$metropolitanstatisticalareamicropolitanstatisticalarea)
head(msaecon)
summary(msaecon)
```

Let's see the geographic variation in these economic indicators:
```{r, results='hide'}
library(tigris)
msa<-core_based_statistical_areas(cb=T)
msa_ec<-geo_join(msa, msaecon, "CBSAFP", "ids", how="inner")

```

```{R}
library(RColorBrewer)
library(sp)
spplot(msa_ec, "gini", at=quantile(msa_ec$gini), col.regions=brewer.pal(n=6, "Greys"), col="transparent")
spplot(msa_ec, "pvacant", at=quantile(msa_ec$pvacant), col.regions=brewer.pal(n=6, "Greys"), col="transparent")
spplot(msa_ec, "medhouse", at=quantile(msa_ec$medhouse), col.regions=brewer.pal(n=6, "Greys"), col="transparent")

```

Merge the MSA data to the BRFSS data

```{r joindata}

joindata<-merge(brfss16m, msaecon, by.x="mmsa",by.y="ids", all.x=T)
joindata$bmiz<-as.numeric(scale(joindata$bmi, center=T, scale=T))
#joindata<-joindata[complete.cases(joindata[, c("bmiz", "race_eth", "agec", "educ", "gini")]),]
#and merge the data back to the kids data
library(dplyr)
joindata<-joindata%>%
  select(bmiz, obese, mmsa, agec, educ, race_eth,smoke, healthmdays, badhealth,bmi,gini, ppoverty, ppovertyz,giniz,medhouse,medhousez, pvacant, pvacantz, male, mmsawt, mmsaname )%>%
  filter(complete.cases(.))


head(joindata[, c("bmiz", "male", "agec", "educ", "gini","medhouse", "pvacant", "mmsa")])

meanbmi<-mean(joindata$bmi, na.rm=T)
sdbmi<-sd(joindata$bmi, na.rm=T)


```

As a general rule, I will do a basic fixed-effects ANOVA as a precursor to doing full multi-level models, just to see if there is any variation amongst my higher level units (groups). If I do not see any variation in my higher level units, I generally will not proceed with the process of multi-level modeling.

```{r anova,message=FALSE, warning=FALSE}
fit.an<-lm(bmiz~as.factor(mmsa), joindata)
anova(fit.an)
#use glm() for non-normal outcomes
fit.ob<-glm(obese~as.factor(mmsa),family=binomial, joindata)
anova(fit.ob, test="Chisq")
```
So we see significant variation in our outcomes across the higher level units.

Now we fit the hierarchical model
```{r, message=FALSE}
library(lme4)
library(lmerTest)
library(arm)
```

Basic hierarchical model for means of each MSA:

```{r}

fit<-lmer(bmiz~(1|mmsa), data=joindata)
arm::display(fit, detail=T)
joindata$predbmi<-sdbmi*(fitted(fit))+meanbmi


citymeans<-aggregate(cbind(bmi, predbmi)~mmsaname,joindata, mean)
head(citymeans, n=10)

plot(predbmi~bmi, citymeans)
```

Model with individual level predictors:

```{r}
fit2<-lmer(bmiz~male+agec+educ+(1|mmsa), data=joindata,  na.action=na.omit)
arm::display(fit2, detail=T)

```

We do a liklihood ratio test to see if our individual level variables are explaining anything about out ourcome:

```{r}
anova(fit, fit2)

```

The do.


We also typically would test to see if the random effect term is significant, SAS pumps out a test of this, so we do the same kind of thing in R
```{r}

rand(fit2)

```

Which shows significant variation in average BMI across MSAs.

###Fitting the basic multi-level model

* The random intercept model assumes you have:
  + *j groups (j=1 to J)*
  + *i individuals within the j groups (i=1 to $n_j$)*
  + for each individual in the *j* groups you have measured $y_{ij}$ and $x_{ij}$ 
and *potentially* for each group *j*, we have may have measured $z_j$ which is a covariate measured at the group level
For example, you do a survey on health and you measure:
+ y = the health status of each individual
+ x= SES, race, etc of each individual
+ j = the higher level each individual lives in, and
+ z = the poverty rate or median income in the higher level


To specify a random intercept model for higher levels, we add, a model term that is (1|HIGHER LEVEL VARIABLE), which tells R to fit only a random intercept for each higher levely, in our case it will be `(1|mmsa)`

```{r}
fit.mix<-lmer(bmiz~male+agec+educ+(1|mmsa), data=joindata)
#do a test for the random effect
rand(fit.mix)
display(fit.mix, detail=T)

```

So we see that our standard deviation at the MSA level is `r round(sqrt(VarCorr(fit.mix)$mmsa[1]), 3)`, and the standard deviation at the individual level (residual standard deviation) is .98. 

Square these to get variances, of course.

Our fixed effects are interpreted as normal,males have higher average bmi's than females, older people have higher BMI's than younger people, those with less than High School education have higher BMI's, while people with college education have lower BMI's than those with a high school education only. **See, just like ordinary regression.**

Some may be interested in getting the intra-class correlation coefficient. While I don't usually pay attention to this, here it is:
```{r ICC}
#it can be a little hairy to get it, but it can be done using various part of VarCorr()
ICC1<-VarCorr(fit)$mmsa[1]/( VarCorr(fit)$mmsa[1]+attr(VarCorr(fit), "sc"))
ICC1
```

So less than 1% of the variance in BMI is due to difference between MSAs. That's not much, but according to our random effect testing, it's not, statistically speaking, 0.

Sometimes, to gain the appreciation, we may want to plot the random effects, I first show the fixed effects, then the random effects:

```{r}
#I need to rescale the estimates to the BMI scale from the z-score scale


fixcoef<-fit.an$coefficients[1]+fit.an$coefficients[-1]
fixcoef<-(fixcoef*sdbmi)+meanbmi

plot(NULL,ylim=c(20, 30), xlim=c(0,1), 
     ylab="Intercept", xlab="") # get the ylim from a summary of rancoefs1
title(main="Fixed Effect Model")

for (i in 1: length(fixcoef)[1]){
  #I plug in a beta, here it's the effect of age from fit.mix
  abline(a=fixcoef[i], b=0,  lwd=1.5, col="green")
}


#It may be easier to visualize the random intercepts by plotting them
rancoefs1<-ranef(fit.mix)$mmsa+fixef(fit.mix)[1]
rancoefs1<-(rancoefs1*sdbmi)+meanbmi
summary(rancoefs1)
plot(NULL,ylim=c(20,30), xlim=c(0,1),
     ylab="Intercept", xlab="Age") # get the ylim from a summary of rancoefs1
title(main="Random Intercept Models")

for (i in 1: length(rancoefs1[,1])){
  #I plug in a beta, here it's the effect of age from fit.mix
  abline(a=rancoefs1[i,1], b=.0,  lwd=1.5, col="maroon")
}

```




###Multilevel model with group-level predictors
Now, I fit the same model above, but this time I include a predictor at the MSA level, the median year houseing was built.

```{r}
#Now I estimate the multilevel model including the effects for the
#MSA level variables
fit.mix2<-lmer(bmiz~male+agec+educ+medhousez+(1|mmsa), data=joindata)
display(fit.mix2, detail=T)
rand(fit.mix2)
ICC2<-VarCorr(fit.mix2)$mmsa[1]/( VarCorr(fit.mix2)$mmsa[1]+attr(VarCorr(fit.mix2), "sc"))
ICC2

#compare the random intercept ad multilevel model with a LRT
anova(fit.mix, fit.mix2)


```

In this case, we see a significant effect of adding the houseing age variable to the model. This suggests that the housing environment is associated with BMI, net of individual  level factors. 

###Random slope models

* We expand the random effect model to include group-varying slopes by:

$$\begin{aligned}
\ E( Y) = \beta_{0j}+ \beta_{ j} x\\
\beta_{0j} = \beta_0 + u_{1j}\\
\beta_{j} = \beta + u_{2j}\\
\end{aligned}\\$$

Where $\beta_0$ is the average intercept, and $u_{1j }$ is the group-specific deviation in the intercept
And where  $\beta$  is the average slope, and $u_{2j }$ is the group-specific deviation in the average slope


This effectively allows one or more of the individual level variables to have different effects in each of the groups. 

```{r}
#To do a random slope model, I do:
fit.mix3<-lmer(bmiz~male+agec+educ+(male|mmsa), joindata, REML=F)
#fit.mix3<-refit(fit.mix3)
display(fit.mix3, detail=T)

#compare the models with a LRT
anova(fit.mix, fit.mix3)
#the random slope model fits better

#plot random slopes and intercepts
rancoefs2<-ranef(fit.mix3)

plot(NULL, ylim=c(-.8, 0), xlim=c(0,1),ylab="Intercept", xlab="Male Gender == 1")
title (main="Random Slope and Intercept Model - Male Gender by Metro")

cols<-rainbow(length(unique(joindata$mmsa)))
for (i in 1: dim(rancoefs2$mmsa)[1]){
  
  abline(a=fixef(fit.mix3)["(Intercept)"]+rancoefs2$mmsa[[1]][i],
         b=fixef(fit.mix3)["male"]+rancoefs2$mmsa[, "male"][i], col=cols[i])
}
```


###Cross level interaction effects
Here, I show the model for a cross-level interaction. This model fits an interaction term between (at least) one individual level variable and a group level variable. 
This allows you to ask very informative questions regarding individuals within specific contexts.

```{r}
#Cross-level interaction model:
fit.mix4<-lmer(bmiz~male+agec+educ+medhousez*race_eth+(1|mmsa), joindata, REML=F)
display(fit.mix4, detail=T)
```

This basically says that blacks in counties with more recently built housing (mean+sd = 1982) have higher BMI's than NH Whites living in areas with average age housing (mean = 1973)

```{r}
#compare the models with a LRT
anova(fit.mix, fit.mix4)

```

Again, the cross level interaction model shows a significantly better fit than the model with only the individual level effects

###Comparing estimates from the linear mixed model to traditional estimates

####Shrinkage
In mixed models, we observe an effect referred to as "shrinkage" of the estimates of the means for each group. 

This term refers to how group-level estimates in multi level models use information from all groups to produce the individual group estimates. 

So, if we have a group with a larger sample size, the estimate of the mean in that group will have lots of information, and low variance in the estimate. 

While a group with a small sample size will have less information about the mean for that group, and generally a higher variance estimate of the mean. Following Gelman and Hill p477, if we have a multilevel model where

$$y_i \sim N \left( \alpha_{j[i]}, \sigma_y^2 \right)$$
and $$\alpha_{j} \sim N\left( \mu_{\alpha}, \sigma_{\alpha}^2 \right)$$
and $n_j$ is the sample size within each group. The multilevel estimate of the mean in each group, $\alpha_j$ is 

$$\alpha_j^{multilevel} = \omega_j \mu_{\alpha} + (1-\omega_j) \bar y_j$$
where
$$\omega_j = 1- \frac{\sigma_{\alpha}^2}{\sigma_{\alpha}^2+\sigma_{y}^2/n_j} $$

is a pooling, or weighting factor. If $\omega$ = 1 the we have complete pooling, and the group means equal the population mean, or when $\omega$=0, we have no pooling and the group means are totally defined with no contribution of the overall mean. This factor $1-\omega_j$ is called the *shrinkage* factor, and describes how much information about each group mean is contributed from the overall population mean, versus the means of each group individually.

So what's the difference?  It may be informative to plot the estimated BMI's for each MSA from the OLS and multilevel models. This section illustrates the effects of "pooling" as Gelman & Hill ch 12. 

```{r}
#these models are good for estimating group means better than traditional methods
#this follows the examples in chapter 12 of Gelman and Hill, I stole the code directly from them.

#complete pooling, this model fits the grand mean ONLY
fit.cp<-lm(bmi~1, joindata)
display(fit.cp)


#No pooling i.e. fixed effects regression, this model fits separate means for each MSA using OLS
lm.unpooled<-lm(bmi~factor(mmsa)-1, joindata)


#partial pooling, this model fits the population mean and MSA deviations using multilevel models
fit0<-lmer(bmi~1+(1|mmsa), joindata)

#partial pooling with covariate
fit0.1<-lmer(bmi~ppovertyz+giniz+(1|mmsa), joindata)


#Plot the means of the counties
J<-length(unique(joindata$mmsa))
ns<-as.numeric(table(fit0.1@frame$mmsa))
sample.size <- as.numeric(table(fit0.1@frame$mmsa))
sample.size.jittered <- sample.size*exp (runif (J, -.1, .1))

par (mar=c(5,5,4,2)+.1)
plot (sample.size.jittered, coef(lm.unpooled), cex.lab=1.2, cex.axis=1.2,
      xlab="sample size in MSA j", 
      ylab=expression (paste("est. intercept, ", alpha[j], "   (no pooling)")),
      pch=20, log="x", ylim=c(25, 30), yaxt="n", xaxt="n")
axis (1, quantile(ns), cex.axis=1.1)
axis (2, seq(25, 30), cex.axis=1.1)
for (j in 1:J){
  lines (rep(sample.size.jittered[j],2),
         coef(lm.unpooled)[j] + c(-1,1)*se.coef(lm.unpooled)[j], lwd=.5)
}
abline (coef(fit.cp)[1], 0, lwd=.5)
title(main="Estimates of MSA Means from the Fixed Effect Model")


#plot MLM estimates of MSA means + se's
par (mar=c(5,5,4,2)+.1)
a.hat.M1 <- coef(fit0)$mmsa[,1]
a.se.M1 <- se.coef(fit0)$mmsa
ns<-as.numeric(table(fit0@frame$mmsa))
plot (as.numeric(ns), t(a.hat.M1), cex.lab=1.2, cex.axis=1.1,
      xlab="sample size in MSA j",
      ylab=expression (paste("est. intercept, ", alpha[j], "(multilevel model)")),
      pch=20, log="x", ylim=c(25, 30), yaxt="n", xaxt="n")
axis (1, quantile(ns), cex.axis=1.1)
axis (2, seq(25,30), cex.axis=1.1)
for (j in 1:length(unique(joindata$mmsa))){
  lines (rep(as.numeric(ns)[j],2),
         as.vector(a.hat.M1[j]) + c(-1,1)*a.se.M1[j], lwd=.5, col="gray10")
}
abline (coef(fit.cp)[1], 0, lwd=.5)
title(main="Estimates of MSA Means from the MLM")


#plot MLM estimates of MSA means + se's, model with MSA covariates
par (mar=c(5,5,4,2)+.1)
a.hat.M2 <- coef(fit0.1)$mmsa[,1]
a.se.M2 <- se.coef(fit0.1)$mmsa
ns<-as.numeric(table(fit0.1@frame$mmsa))
plot (as.numeric(ns), t(a.hat.M2), cex.lab=1.2, cex.axis=1.1,
      xlab="sample size in MSA j",
      ylab=expression (paste("est. intercept, ", alpha[j], "(multilevel model with covariates)")),
      pch=20, log="x", ylim=c(25, 30), yaxt="n", xaxt="n")
axis (1, quantile(ns), cex.axis=1.1)
axis (2, seq(25,30), cex.axis=1.1)
for (j in 1:length(unique(joindata$mmsa))){
  lines (rep(as.numeric(ns)[j],2),
         as.vector(a.hat.M2[j]) + c(-1,1)*a.se.M2[j], lwd=.5, col="gray10")
}
abline (coef(fit.cp)[1], 0, lwd=.5)
title(main="Estimates of MSA Means from the MLM with MSA predictors")

```

In the model with no pooling, we see greater variance in the estimates for MSAs with smaller sample sizes, although not terribly variable in this case since we have lots of data, and in the model with pooling, the variance in these estimates is reduced, becuase we are using the estimates from the MSAs with lots of information to estimate the population mean of the MSAs with great precision.



