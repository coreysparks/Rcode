---
title: "DEM 7283 - Example 2- Logit and Probit Models"
author: "Corey Sparks, PhD"
date: "January 29, 2018"
output:
  html_document:
    fig_height: 7
  pdf_document: default
  word_document: default
---
In the vast majority of situations in your work as demographers, your outcome will either be of a qualitative nature or non-normally distributed, especially if you work with individual level survey data. 

When we speak of qualitative outcomes, we generally are concerned with the observation of:

* A particular behavior (marriage, migration, birth, death)

* A transition (unemployed to employed, married to divorced)

* A threshold characteristic (adoption of sterilization after ideal # of children is reached)

* In general, each of these outcomes would be coded as a binary variable (1 or 0) depending on whether the outcome of interest was observed


###Example
This example will cover the use of R functions for fitting binary logit and probit models to complex survey data.

For this example I am using  2016 CDC Behavioral Risk Factor Surveillance System (BRFSS) SMART metro area survey data. [Link](https://www.cdc.gov/brfss/smart/smart_2016.html)

```{r "setup", include=FALSE}
#load brfss
library(car)
library(stargazer)
library(survey)
library(questionr)
load("~/Google Drive/classes/dem7283/class18/data/brfss16_mmsa.Rdata")

#The names in the data are very ugly, so I make them less ugly
nams<-names(brfss16m)
head(nams, n=10)
#we see some names are lower case, some are upper and some have a little _ in the first position. This is a nightmare.
newnames<-tolower(gsub(pattern = "_",replacement =  "",x =  nams))
names(brfss16m)<-newnames
#Poor or fair self rated health
#brfss16m$badhealth<-ifelse(brfss16m$genhlth %in% c(4,5),1,0)
brfss16m$badhealth<-recode(brfss16m$genhlth, recodes="4:5=1; 1:3=0; else=NA")
#race/ethnicity
brfss16m$black<-recode(brfss16m$racegr3, recodes="2=1; 9=NA; else=0")
brfss16m$white<-recode(brfss16m$racegr3, recodes="1=1; 9=NA; else=0")
brfss16m$other<-recode(brfss16m$racegr3, recodes="3:4=1; 9=NA; else=0")
brfss16m$hispanic<-recode(brfss16m$racegr3, recodes="5=1; 9=NA; else=0")

brfss16m$race_eth<-recode(brfss16m$racegr3, recodes="1='nhwhite'; 2='nh black'; 3='nh other';4='nh multirace'; 5='hispanic'; else=NA", as.factor.result = T)


#insurance
brfss16m$ins<-recode(brfss16m$hlthpln1, recodes ="7:9=NA; 1=1;2=0")

#income grouping
brfss16m$inc<-ifelse(brfss16m$incomg==9, NA, brfss16m$incomg)

#education level
brfss16m$educ<-recode(brfss16m$educa, recodes="1:2='0Prim'; 3='1somehs'; 4='2hsgrad'; 5='3somecol'; 6='4colgrad';9=NA", as.factor.result=T)
brfss16m$educ<-relevel(brfss16m$educ, ref='2hsgrad')

#employment
brfss16m$employ<-recode(brfss16m$employ1, recodes="1:2='Employed'; 2:6='nilf'; 7='retired'; 8='unable'; else=NA", as.factor.result=T)
brfss16m$employ<-relevel(brfss16m$employ, ref='Employed')

#marital status
brfss16m$marst<-recode(brfss16m$marital, recodes="1='married'; 2='divorced'; 3='widowed'; 4='separated'; 5='nm';6='cohab'; else=NA", as.factor.result=T)
brfss16m$marst<-relevel(brfss16m$marst, ref='married')

#Age cut into intervals
brfss16m$agec<-cut(brfss16m$age80, breaks=c(0,24,39,59,79,99))

#BMI, in the brfss16ma the bmi variable has 2 implied decimal places, so we must divide by 100 to get real bmi's

brfss16m$bmi<-brfss16m$bmi5/100

#smoking currently
brfss16m$smoke<-recode(brfss16m$smoker3, recodes="1:2='Current'; 3='Former';4='NeverSmoked'; else=NA", as.factor.result=T)
brfss16m$smoke<-relevel(brfss16m$smoke, ref = "NeverSmoked")

```

###Analysis
First, we will do some descriptive analysis, such as means and cross tabulations.
```{r}
library(dplyr)
sub<-brfss16m%>%
  select(badhealth,mmsaname, bmi, agec,race_eth, marst, educ,white, black, hispanic, other, smoke, ins, mmsawt, ststr) %>%
  filter( complete.cases(.))

#First we tell R our survey design
options(survey.lonely.psu = "adjust")
des<-svydesign(ids=~1, strata=~ststr, weights=~mmsawt, data =sub )

```


First, we examine the % of US adults with poor/fair health by education level, and do a survey-corrected chi-square test for independence.

```{r}
library(ggplot2)
cat<-svyby(formula = ~badhealth, by = ~educ, design = des, FUN = svymean, na.rm=T)
svychisq(~badhealth+educ, design = des)


qplot(x=cat$educ,y=cat$badhealth, data=cat ,xlab="Education", ylab="%  Fair/Poor Health" )+
geom_errorbar(aes(x=educ, ymin=badhealth-se,ymax= badhealth+se), width=.25)+
ggtitle(label = "% of US Adults with Fair/Poor Health by Education")

#calculate race*health cross tabulation, and plot it
dog<-svyby(formula = ~badhealth, by = ~race_eth, design = des, FUN = svymean, na.rm=T)
svychisq(~badhealth+race_eth, design = des)
qplot(x=dog$race_eth,y=dog$badhealth, data=dog ,xlab="Race", ylab="%  Fair/Poor Health" )+
geom_errorbar(aes(x=race_eth, ymin=badhealth-se,ymax= badhealth+se), width=.25)+
ggtitle(label = "% of US Adults with Fair/Poor Health by Race/Ethnicity")

#calculate race*education*health cross tabulation, and plot it
catdog<-svyby(formula = ~badhealth, by = ~race_eth+educ, design = des, FUN = svymean, na.rm=T)
catdog
#this plot is a little more complicated
catdog$race_rec<-rep(c("NHWhite","Hispanic", "NHBlack","NH Multi", "NH Other" ),5)
catdog$educ_rec<-factor(c(rep("Primary Sch", 5), rep("LT HS", 5), rep("HS Grad", 5), 
                          rep("Some College", 5), rep("College Grad", 5)), ordered = T)
#fix the order of the education factor levels
catdog$educ_rec<-factor(catdog$educ_rec, levels(catdog$educ_rec)[c(4,3,2,5,1)])

p<-ggplot(catdog, aes(educ_rec,badhealth,),xlab="Race", ylab="% Bad Health")
p<-p+geom_point(aes(colour=race_rec))
p<-p+geom_line(aes(colour=race_rec,group=race_rec))
#p<-p+geom_errorbar(aes(x=educ_rec, ymin=badhealth-se,ymax= badhealth+se,colour=race_rec), width=.25)
p<-p+ylab("% Fair/Poor Health")
p<-p+xlab("Education Level")
p+ggtitle("% of US Adults in 2011 in Bad Health by Race and Education")
```


Which shows a significant variation in health status by education level and race/ethnicty

##Logit and Probit models
If our outcome is dichtomous (1/0), the natural distribution to consider for a GLM is the binomial
$$y \sim \ \text{Binomial}\binom{n}{p}$$
with $p$ being the mean of the binomial, and n being the number of trials, generally when you have individual data, n is always 1, and p is the probability of observing the 1, conditional on the observed predictors. There are two common techniques to estimate the mean, logistic and probit regression. In a Logistic model, the link function is the inverse logit function, or

$\text{Logit}^{-1}(p) =log \frac{p}{(1-p)}$

Which gives us the following conditional mean model:

$$E(y|x)  = \frac{1}{1+ exp({-\sum_k \beta_k x_{ik}})}$$
Which situates the model within the logistic distribution function. Expressing *p* as a linear model is done via this log odds transformation of the probability:

$$log \frac{p}{(1-p)} = \sum_k \beta_k x_{ik}$$

For the Probit model, the link function is the inverse cumulative Normal distribution:

$$E(y|x) = \Phi^{-1}(p) = \Phi (\sum_k \beta_k x_{ik})$$

In practice, these two models give very similar estimates and will have very similar interpretations, although the logitistic regression model has the more convenient odds ratio interpretation of its $\beta's$, while the probit model's coefficients are often transformed into marginal coefficients, which is more of a challenge and software generally doesn't give you these by default. 


##Logit/Probit Regression example
There is no trick to fitting logistic regression models usign survey data, just use the `svyglm()` function with the apppriate distribution specified via `family=binomial` for logistic and `family=binomial(link="probit")` for the probit model. You don't have to specify the link function if you're just doing the logistic model, as it is the default. 


```{r pandertables}
#Logit model
fit.logit<-svyglm(badhealth~race_eth+educ+agec,design= des, family=binomial)


#probit model
fit.probit<-svyglm(badhealth~race_eth+educ+agec, design=des, family=binomial(link= "probit"))
```



#Present both model coefficients next to one another
```{r, results='asis'}
stargazer(fit.logit, fit.probit,type = "html", style="demography",covariate.labels=c("Black","MultRace" ,"Other","NHWhite", "PrimarySchool", "SomeHS","SomeColl", "CollGrad", "Age 24-39","Age 39-59" ,"Age 59-79", "Age 80+"))
```

Both of these models show the exact same patterns of effects, with Hispanics, blacks and multi-race individuals showing increased chances of reporting poor/fair health, when compared to whites (Reference group). Similarly, the education variables shows a negative linear trend, with those with more education having lower chances of reporting poor/fair health compared to those with a primary school education (Reference group), and likewise, as people get older, they are more likely to report poor/fair health, compared to those under age 24 (Reference group).


##Marginal effects 
In a regression model in general, the $\beta's$ are the solution to the differential equation:
$$\frac{\partial y}{\partial x} = \beta$$

which is just the rate of change in y, given x, known as the marginal effect. This is the case for *strictly linear model*

In the logit and probit model, which are nonlinear models, owing to their presumed model structure, the marginal effect also has to take into account the change in the respective pdf with respect to the mean, or:

$$\frac{\partial y}{\partial x} = \beta *\frac{\partial \Phi(x' \beta)}{\partial x'\beta}$$

So we have to multiply the estimated $\beta$ by the p.d.f. of the assumed marginal distribution evaluated at the mean function. In R that's not big problem:

```{r}
#Logit marginal effects
log.marg<-coef(fit.logit)*mean(dlogis(predict(fit.logit)), na.rm=T)

#for probit now
prob.marg<-coef(fit.probit)*mean(dnorm(predict(fit.probit)), na.rm=T)

plot(log.marg[-1], ylab="Marginal Effects", axes=T,xaxt="n", main="Marginal Effects from Logit and Probit models", ylim=c(-.25, .2))
axis(side=1, at=1:13, labels=F)
text(x=1:13, y=-.3,  srt = 45, pos = 1, xpd = TRUE,
     labels = c( "Hispanic", "Black","MultRace" ,"Other",
                 "SomeHS","HS Graduate", "SomeColl", "CollGrad", "Age 24-39","Age 39-59" ,"Age 59-79", "Age 80+" ))
points(prob.marg[-1], col=2)
abline(h=0, col=3)
legend("bottomright", legend=c("Logit Model", "Probit Model"), col=c("black", "red"),pch=1)
```

Which shows us that the marginal effects are very similar between the two models. We can coax these into a table like:

```{r}
data.frame(m.logit=log.marg, m.probit=prob.marg)
```

##Fitted Values
As I often say, I like to talk about "interesting cases". In order to do this, you need the fitted mean for a particular case. This is done by getting the fitted values for that case from the model. To do this, I generate a bunch of "fake people" that have variability in the model covariates, and fit the model for each type of person. This is perhaps overkill in this example because I fit every type of person, ideally you would want a few interesting cases to discuss.

In order to derive these, we effectively "solve the equation" for the model, or another way of saying it, we estimate the conditional mean of y, by specifying the x values that are meaningful for a particular comparison.
For example the probabilty of a white, young college educated person reporting poor health is just the estimate of the model, evaluated at those particular characteristics:

$$\text{Pr(poor/fair health)} =  \frac{1}{1+exp({\beta_0 + \beta_1*white + \beta_2*young+\beta_3*college})}$$


```{r}
#get a series of predicted probabilites for different "types" of people for each model
#expand.grid will generate all possible combinations of values you specify
dat<-expand.grid(race_eth=levels(brfss16m$race_eth), educ=levels(brfss16m$educ), agec=levels(brfss16m$agec))

#You MAY need to get rid of impossible cases here

#generate the fitted values
fit<-predict(fit.logit, newdat=dat,type="response")
fitp<-predict(fit.probit, newdat=dat,type="response")
#add the values to the fake data
dat$fitted.prob.lrm<-round(fit, 3)
dat$fitted.prob.pro<-round(fitp, 3)

#Print the fitted probabilities for the first 20 cases
head(dat, n=20)

```
Which show us the estimated probabilty of reporting poor/fair health for each specified type of "fake person" that we generate. For example, let's look at the probability for a Non-Hispanic white, age 39-59 with a college education, compared to a Hispanic person, age 39-59 with a primary school education:

```{r}

dat[which(dat$race_eth=="nhwhite"&dat$agec=="(39,59]"&dat$educ=="4colgrad"),]
dat[which(dat$race_eth=="hispanic"&dat$agec=="(39,59]"&dat$educ=="0Prim"),]
```
The first case has an estimated probability of reporting poor/fair health of about 7%, while the second case has over about a 50% chance. These are often more effective ways to convey the result of a model, instead of talking about all the regression coefficients. 


The probablity that a nh white person who is <25 years old and has a high school educaiton is `r round( dat[which(dat$race_eth=="nhwhite"&dat$agec=="(0,24]"&dat$educ=="2hsgrad"), "fitted.prob.lrm"], 3)  `

##Nested model comparison
Often in a research setting we are interested in comparing several models, and almost never are we satisfied with a single solitary model. This is because the literature on our subjects often has multiple facets to it. So, for instance, we may be interested in how SES mediates the effects of race/ethnicity on health (see [Shuey and Wilson 2008](http://journals.sagepub.com/doi/abs/10.1177/0164027507311151) and [Sudano and Baker 2005](https://www.ncbi.nlm.nih.gov/pubmed/16055252) or [Hummer 1993](http://www.jstor.org/stable/pdf/2579860.pdf)  for a few examples of these types of analysis). 


Typically in these types of analysis, predictor variables are entered into the model in "blocks". For example, let's look at the self-rated health outcome (0=good or excellent health, 1= fair/poor health) from [last week](http://rpubs.com/corey_sparks/245830). But instead of entering all variables in the model simultaneously, we begin with the effect of race/ethnicity, then add the effect of SES then the effects of health behaviour variables.

```{r }
fit.logit1<-svyglm(badhealth~race_eth,design= des, family=binomial) #race only
fit.logit2<-svyglm(badhealth~race_eth+educ,design= des, family=binomial) #race+education
fit.logit3<-svyglm(badhealth~race_eth+educ+ins+smoke,design= des, family=binomial)#race+education+health behaviors

summary(fit.logit1)
```


In model 1 we see that Hispanics and blacks have a higher odds of reporting poor self rated health, compared to non-Hispanic whites, while the "other" group shows lower odds of reporting poor health.

Now, let's see if, by controlling for education, some of these differences go away, or are reduced. The fancy word for when an effect is reduced is "attenuated". We will also do a test to see if the model with the education term significantly improves the model fit. Traditionally this would be done using a likelihood ratio test, but in survey models, that's not kosherload

```{r}
summary(fit.logit2)
regTermTest(fit.logit2, test.terms = ~educ, method="Wald", df = NULL)
```

so, we see the race effects in all groups attenuate (reduce in size) somewhat after considering education, so the differences in health are smaller once you control for education.

The F test also suggest that the second model fits the data better than the first. It is another of these omnibus tests that asks whether there is any variation in our outcome by education in the second model.

Next we consider the third model, which contains health behaviors of current smoking and insurance coverage:

```{r}
summary(fit.logit3)
regTermTest(fit.logit3, test.terms=~ins+smoke, method="Wald", df = NULL)
```

In this model, we see the effects for Hispanics and blacks go back up. This is somewhat confusing, but is most likely related to the differential rates of smoking among those groups, compared to whites. Both current and former smokers are more likely to report poor/fair health, while insurance coverage does not affect the odds at all. Finally, we see that the model is significantly better fitting than the previous model. 


```{r, results='asis'}

stargazer(fit.logit1, fit.logit2, fit.logit3,type = "html", style="demography", covariate.labels =c("Black","MultRace" ,"Other","NHWhite", "PrimarySchool", "SomeHS","SomeColl", "CollGrad",  "Insurance", "Current Smoker", "Former Smoker") )

```

##Stratified models
Often in the literature, we will see models stratified by some predictor. This is usually because a specific hypothesis is stated regarding how the effect of a particular predictor varies by some categoricial variable. In this case, we may be interested in considering if education or smoking universally affects the poor health outcome. We get at this by *stratifying* our analysis by race (in this example).

The easiest way to do this is to subset the data by race and run the models separately. 

**The first thing we do** is test for the interaction of education and race. If this interaction is not significant, we have no justification for proceeding, becuase the effect of education does not vary by race group. **This is the test of parallel slopes, a' la the ANCOVA model**

```{r}
fit.logitint<-svyglm(badhealth~race_eth*educ+ins+smoke,design= des, family=binomial)#race*education interaction+health behaviors
regTermTest(fit.logitint, test.terms = ~race_eth:educ, method = "Wald", df=NULL)
```

Here, the F-test does indicate that the interaction term in the model is significant, so the effects of education are not constant by race. 

Now we stratify our models:

```{r}
fit.unrestricted<-svyglm(badhealth~educ+ins+smoke,design= des, family=binomial)
fit.logit.white<-svyglm(badhealth~(educ+ins+smoke),design= subset(des, white==1), family=binomial)
fit.logit.black<-svyglm(badhealth~(educ+ins+smoke),design= subset(des, black==1), family=binomial)
fit.logit.other1<-svyglm(badhealth~(educ+ins+smoke),design= subset(des, race_eth=="nh other"), family=binomial)
fit.logit.other2<-svyglm(badhealth~(educ+ins+smoke),design= subset(des, race_eth=="nh multirace"), family=binomial)
fit.logit.hisp<-svyglm(badhealth~(educ+ins+smoke),design= subset(des, hispanic==1), family=binomial)

```

Here we examine the model results next to one another
```{r, results='asis'}
stargazer(fit.logit.white, fit.logit.black, fit.logit.hisp, fit.logit.other1, fit.logit.other2,type = "html", style="demography", covariate.labels =c("Intercept",  "Some HS","HS Graduate", "Some College", "College Grad", "Insurance", "Current Smoker", "Former Smoker") )

```


```{r}
beta.test<-function(model1, model2, betaname){
s1<-summary(model1)$coef
s2<-summary(model2)$coef
db <- ((s2[rownames(s2)==betaname,1]-s1[rownames(s1)==betaname,1]))^2
sd <-s2[rownames(s2)==betaname,2]^2+s1[rownames(s1)==betaname,2]^2
td <- db/sd
beta1=s1[rownames(s1)==betaname,1]
beta2=s2[rownames(s2)==betaname,1]
pv<-1- pchisq(td, df = 1)
print(list(beta=betaname,beta1=beta1, beta2=beta2, x2=td, pvalue=pv))
}
```

Here is an example of testing if the "Current Smoking" effect is the same among whites and blacks. This follows the logic set forth in [Allison 2010, p 219](https://books.google.com/books?id=RmbZ2y1KLwUC&q=219#v=snippet&q=219&f=false)

Test for $\beta_{1j} = \beta_{1k}$ in two models $j \text{ and } k$
$$z= \frac{\beta_{1j} - \beta_{1k}}{\left[ s.e.(\beta_{1j}) \right]^2+\left[ s.e.(\beta_{1k}) \right]^2}$$


compare the $|z|$ test to a normal distribution for the p value. 

```{r}
beta.test(fit.logit.white, fit.logit.black, "smokeCurrent")
```
Which suggests that the effect of current smoking is the same in the two groups.

Here we also examine the equality of the college education effect, this time between Hispanics and non hispanic multirace
```{r}
beta.test(fit.logit.hisp, fit.logit.other2, "educ4colgrad")
```
Which suggests the effect of college education is the same in these two groups.
